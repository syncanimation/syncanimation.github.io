# SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation 

[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)  
[Project Page](https://syncanimation.github.io/) | [Paper (arXiv)](https://arxiv.org/abs/2501.14646)   

> â€œGenerating talking avatar driven by audio remains a significant challenge. Existing methods typically require high computational costs and often lack sufficient facial detail and realism, making them unsuitable for applications that demand high real-time performance and visual quality. Additionally, while some methods can synchronize lip movement, they still face issues with consistency between facial expressions and upper body movement, particularly during silent periods. In this paper, we introduce SyncAnimation, the first NeRF-based method that achieves audio-driven, stable, and real-time generation of speaking avatar by combining generalized audio-to-pose matching and audio-to-expression synchronization. By integrating AudioPose Syncer and AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression generation, progressively producing audio-synchronized upper body, head, and lip shapes. Furthermore, the High-Synchronization Human Renderer ensures seamless integration of the head and upper body, and achieves audio-sync lip.â€  

<!--
## ğŸ§  ç®€ä»‹  

è¯­éŸ³é©±åŠ¨çš„äººè„¸åˆæˆåœ¨å§¿æ€ã€åŒæ­¥æ€§ã€ç»†èŠ‚è¿˜åŸç­‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚**SyncTalk** çš„æ ¸å¿ƒåœ¨äº**åŒæ­¥æ€§**ï¼ˆlip sync + è¡¨æƒ… + å¤´éƒ¨è¿åŠ¨ä¸€è‡´æ€§ï¼‰æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒSyncTalk é‡‡ç”¨ä»¥ä¸‹æœºåˆ¶ï¼š

- åŸºäºä¸‰å¹³é¢å“ˆå¸Œ (tri-plane hash) è¡¨ç¤ºæ¥ä¿æŒèº«ä»½ä¸€è‡´æ€§  
- åŒæ—¶ç”ŸæˆåŒæ­¥å£å‹ã€é¢éƒ¨è¡¨æƒ…å’Œç¨³å®šå¤´éƒ¨å§¿æ€  
- åœ¨é«˜åˆ†è¾¨ç‡è§†é¢‘ä¸­æ¢å¤å¤´å‘ç­‰ç»†èŠ‚  
-->

## ğŸ›  å®‰è£…ä¸ä¾èµ–  

### Linux / Ubuntu  

ä»¥ä¸‹ä¸ºåœ¨ Ubuntu ä¸Šçš„æ¨èå®‰è£…æµç¨‹ï¼ˆå·²çŸ¥åœ¨ Ubuntu 18.04 + PyTorch 1.12.1 + CUDA 11.3 ä¸‹è¿›è¡Œæµ‹è¯•ï¼‰ï¼š

```bash
git clone https://github.com/ZiqiaoPeng/SyncTalk.git
cd SyncTalk

# å»ºè®®ä½¿ç”¨ conda ç¯å¢ƒ
conda create -n synctalk python==3.8.8
conda activate synctalk

# å®‰è£… PyTorch ä¸ torchvisionï¼ˆæ ¹æ® CUDA ç‰ˆæœ¬é€‰æ‹©å¯¹åº”ç‰ˆæœ¬ï¼‰
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113

sudo apt-get install portaudio19-dev
pip install -r requirements.txt

# å®‰è£…ä¾èµ–æ¨¡å—ï¼ˆfreqencoder / gridencoder / shencoder / raymarchingï¼‰
pip install ./freqencoder
pip install ./shencoder
pip install ./gridencoder
pip install ./raymarching

# å®‰è£… PyTorch3Dï¼ˆè‹¥æœ‰å›°éš¾ï¼Œå¯ä½¿ç”¨è„šæœ¬ fallback æ–¹æ¡ˆï¼‰
pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu113_pyt1121/download.html
# æˆ–è€…ï¼š
python ./scripts/install_pytorch3d.py

# å®‰è£… TensorFlow GPU ç‰ˆæœ¬ï¼ˆéƒ¨åˆ†æ¨¡å—å¯èƒ½ä¾èµ–ï¼‰
pip install tensorflow-gpu==2.8.1
```

> **æç¤º**ï¼šå®‰è£… PyTorch3D æ—¶å¯èƒ½é‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨ `scripts/install_pytorch3d.py` å¤„ç†ã€‚

---

<!--
## ğŸ”„ æ•°æ®å‡†å¤‡  

### é¢„è®­ç»ƒæ¨¡å‹  

è¯·å°†ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¾‹å¦‚ `May.zip`ã€`trial_may.zip`ï¼‰æ”¾å…¥ç›¸åº”ç›®å½•ï¼š

- `data/May.zip` â†’ è§£å‹è‡³ `data/May/`  
- `model/trial_may.zip` â†’ è§£å‹è‡³ `model/trial_may/`  

### è¾“å…¥è§†é¢‘å¤„ç†  

1. ä¸‹è½½ face-parsing æ¨¡å‹  
   ```bash
   wget https://github.com/YudongGuo/AD-NeRF/blob/master/data_util/face_parsing/79999_iter.pth?raw=true -O data_utils/face_parsing/79999_iter.pth
   ```
2. ä¸‹è½½ 3DMM å¤´å§¿ä¼°è®¡æ¨¡å‹  
   ```bash
   wget â€¦  # å¤šä¸ªæ–‡ä»¶ï¼šexp_info.npy, keys_info.npy, sub_mesh.obj, topology_info.npy  
   ```
3. ä¸‹è½½ Basel Face Model (BFM 2009)ï¼Œè½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼  
   ```bash
   # ä¸‹è½½ .mat æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶æ”¾åˆ° data_utils/face_tracking/3DMM/
   cd data_utils/face_tracking
   python convert_BFM.py
   ```
4. è¾“å…¥è§†é¢‘è¦æ±‚ï¼š  
   - å¸§ç‡ 25 FPS  
   - æ¯å¸§åŒ…å«è®²è¯äººé¢éƒ¨  
   - åˆ†è¾¨ç‡æ¨è ~512Ã—512  
   - æ—¶é•¿çº¦ 4â€“5 åˆ†é’Ÿ  

5. æ‰§è¡Œè§†é¢‘å¤„ç†è„šæœ¬  
   ```bash
   python data_utils/process.py data/<ID>/<ID>.mp4 --asr ave
   ```
   - æ”¯æŒ `ave`ã€`deepspeech`ã€`hubert` ä¸‰ç§ç‰¹å¾æå–  
   - å¯é€‰åœ°ï¼Œè¿è¡Œ OpenFace çš„ `FeatureExtraction`ï¼Œç”Ÿæˆçœ¼ç›çœ¨åŠ¨ AU45 ä¿¡æ¯ï¼ˆé‡å‘½åä¸º `data/<ID>/au.csv`ï¼‰  

> æ³¨æ„ï¼šç”±äº EmoTalk çš„ blendshape æ•æ‰æœªå¼€æºï¼Œå› æ­¤è¿™é‡Œé»˜è®¤ä½¿ç”¨ mediapipe çš„ blendshape æ•æ‰ã€‚å¯¹äºæŸäº›åœºæ™¯æ•ˆæœä¸ä½³ï¼Œå¯è€ƒè™‘æ›¿æ¢æˆ–æ”¹è¿›ã€‚  

---

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹  

### è¯„ä¼° / æ¨ç†  

```bash
python main.py data/May --workspace model/trial_may -O --test --asr_model ave
python main.py data/May --workspace model/trial_may -O --test --asr_model ave --portrait
```

- `--portrait`ï¼šå°†ç”Ÿæˆçš„äººè„¸è´´å›åŸå§‹å›¾åƒ â†’ ç”»è´¨è¾ƒå¥½  
- æˆåŠŸè¿è¡Œåå°†è¾“å‡º PSNR / LPIPS / LMD ç­‰æŒ‡æ ‡  

è‹¥è¦ä½¿ç”¨ç›®æ ‡éŸ³é¢‘è¿›è¡Œæ¨ç†ï¼š

```bash
python main.py data/May --workspace model/trial_may -O --test --test_train --asr_model ave --portrait --aud ./demo/test.wav
```

æ³¨æ„ï¼šéŸ³é¢‘éœ€ä¸º `.wav` æ ¼å¼ã€‚å¦‚æœä½¿ç”¨å…¶ä»–ç‰¹å¾ï¼ˆå¦‚ npyï¼‰ï¼Œå¯æ”¹è·¯å¾„è‡³å¯¹åº”æ–‡ä»¶ã€‚

### è®­ç»ƒ  

é»˜è®¤æ–¹å¼ä»ç£ç›˜æŒ‰éœ€åŠ è½½æ•°æ®ï¼š

```bash
python main.py data/May --workspace model/trial_may -O --iters 60000 --asr_model ave
python main.py data/May --workspace model/trial_may -O --iters 100000 --finetune_lips --patch_size 64 --asr_model ave
# æˆ–è€…ä½¿ç”¨è„šæœ¬
sh ./scripts/train_may.sh
```

è‹¥æƒ³åŠ å…¥èº¯å¹²è®­ç»ƒä»¥ä¿®å¤åŒä¸‹å·´ï¼ˆæ³¨æ„ï¼šè®­ç»ƒèº¯å¹²åä¸èƒ½ç”¨ `--portrait` æ¨¡å¼ï¼‰ï¼š

```bash
python main.py data/May/ --workspace model/trial_may_torso/ -O --torso --head_ckpt <head_ckpt>.pth --iters 150000 --asr_model ave
python main.py data/May --workspace model/trial_may_torso -O --torso --test --asr_model ave
python main.py data/May --workspace model/trial_may_torso -O --torso --test --test_train --asr_model ave --aud ./demo/test.wav
```

---

## ğŸ“Š è¯„ä»·æŒ‡æ ‡  

ç¤ºä¾‹ï¼ˆå•äººï¼‰ï¼š

| æ¨¡å¼                      | PSNR    | LPIPS    | LMD     |
|---------------------------|---------|-----------|---------|
| SyncTalk (ä¸è´´å›åŸå›¾)     | 32.201  | 0.0394    | 2.822   |
| SyncTalk (è´´å›åŸå›¾)       | 37.644  | 0.0117    | 2.825   |

ï¼ˆè®ºæ–‡ç»™å‡ºäº†å¤šä¸ªè¢«è¯•çš„å¹³å‡æŒ‡æ ‡ï¼‰

---
 -->

## ğŸ“ Citation  

Please cite the following paper if you use this method, model, or conduct derivative research based on this project:

```tex
@inproceedings{ijcai2025p185,
  title     = {SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation},
  author    = {Liu, Yujian and Xu, Shidang and Guo, Jing and Wang, Dingbin and Wang, Zairan and Tan, Xianfeng and Liu, Xiaoli},
  booktitle = {Proceedings of the Thirty-Fourth International Joint Conference on
               Artificial Intelligence, {IJCAI-25}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {James Kwok},
  pages     = {1657--1665},
  year      = {2025},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2025/185},
  url       = {https://doi.org/10.24963/ijcai.2025/185},
}
```

---

## ğŸ™ Acknowledgements  

This project is built upon or inspired by the following open-source projects:

- Synctalk  
- ER-NeRF  
- GeneFace   
- AD-NeRF  
- Deep3DFaceRecon_pytorch  

We sincerely thank the authors of these projects for their contributions to the open-source community.

---

## âš ï¸ Disclaimer  

By using this project, you agree to comply with all applicable laws and regulations.
You must not use it to generate or disseminate harmful content.
The developers assume no responsibility for any direct, indirect, or consequential damages arising from the use or misuse of this software. 
