<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
<!--   <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
<!--   <meta name="viewport" content="width=device-width, initial-scale=1"> -->
<!--   <title>Audiodynamic: Real-Time Audio-Driven Talking Head and Torso Generation with Disentangled Head Pose and Expressions</title> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation</h1>
<!--           <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yujian Liu<sup>1,2</sup>,</span>
            <span class="author-block">
              Shidang Xu<sup>2</sup>,</span>
            <span class="author-block">
              Jing Guo<sup>1,3</sup>,
            </span>
            <span class="author-block">
              Zairan Wang<sup>1</sup>,
            </span>
            <span class="author-block">
              Xiaoli Liu<sup>1</sup>,
            </span>
          </div> -->

<!--           <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>AiShiWeiLai AI Research, Beijing, China,</span>
            <span class="author-block"><sup>2</sup>South China University of Technology, Guangzhou, China</span>
            <span class="author-block"><sup>3</sup>Beijing Institute of Technology, Beijing, China</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--               <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
<!--               <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Code Link. -->
<!--               <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="abstract">
  <div class="container is-max-desktop">
    <div class="abstract">
      <h1 class="title is-3" style="font-size: 2.1em; font-weight: bold; margin-bottom: 20px; text-align: center;">Abstract</h1>
      <div style="text-align: justify">
        <p>Generating talking avatars driven by audio remains a significant challenge. Existing methods typically require high computational costs and often lack sufficient facial detail and realism, making them unsuitable for applications that demand high real-time performance and visual quality. Additionally, while some methods can synchronize lip movement, they still face issues with consistency between facial expressions and upper body movement, particularly during silent periods.</p>
        
        <p>In this paper, we introduce SyncAnimation, the first NeRF-based method that achieves audio-driven, stable, and real-time generation of speaking avatars by combining generalized audio-to-pose matching and audio-to-expression synchronization. By integrating AudioPose Syncer and AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression generation, progressively producing audio-synchronized upper body, head, and lip shapes. Furthermore, the High-Synchronization Human Renderer ensures seamless integration of the head and upper body, and achieves audio-sync lip movements.</p>
      </div>
    </div>
  </div>
</section>
  
<section class="Audiodynamic">
  <div class="container is-max-desktop">
    <div class="Audiodynamic-body">

      <img src="./static/images/figure1.png" alt="Audiodynamic Image" style="width: 100%; margin-top: 10px; height: auto;">
      <h2 class="text-align: justify" style="width: 100%; text-align: justify; margin-top: 5px;"> 
        SyncAnimation is the first NeRF-based fully generative approach that utilizes audio-driven generation to create expressions and an adjustable torso (left). 
        SyncAnimation requires only audio and monocular, or even noise, to render highly detailed identity information, along with realistic and dynamic facial and torso changes, while maintaining audio consistency (right).
      </h2>
    </div>
  </div>
</section>

<section class="Overall_Pipeline">
  <div class="container is-max-desktop">
    <div class="Overall_Pipeline">
      <h1 class="title is-3" style="font-size: 2.1em; font-weight: bold; margin-bottom: 20px; margin-top: 30px; text-align: center;">Overall Pipeline</h1>
      <img src="./static/images/pipline.png" alt="Audiodynamic Image" style="width: 100%; height: auto;">
      <h2 class="text-align: justify" style="width: 100%; text-align: justify; margin-top: 3px; margin-bottom: 20px;"> 
        SyncAnimation Framework: Given an image and audio, the preprocessing extracts 3DMM parameters for Audio2Pose and Audio2Emotion as references (or noise). 
        It then generates the upper body, head, and lip refinement, ensuring pose consistency and facial expression alignment with the audio.
      </h2>
    </div>
  </div>
</section>



<section class="hero teaser" style="box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3); padding: 20px; border-radius: 8px; width: 70%; margin: 0 auto;margin-bottom: 30px;">
  <div class="container is-max-desktop" style="padding: 0; margin: 0;">
    <div class="hero-body">
      <!-- h2 居中对齐 -->
      <h2 class="title is-3" style="font-size: 2.1em; font-weight: bold; margin-bottom: 20px; text-align: center;">Comparison with SOTA</h2>

      <!-- h3 左对齐，并且去除左右内边距 -->
      <h3 class="title is-4" style="text-align: left; margin: 0; padding: 20px 0px;">Comparison with Gan</h3>
        <!-- We compare with 四个 one-shot methods在: Wav2Lip, DINet (CVPR 2023),IP-LAP and EDTalk. 我们的SyncAnimation实现了动态且清晰的唇型，以及唇音一致性
          --> 
        <h4 class="text-align: justify" style="width: 100%; text-align: justify; margin-bottom: 5px;">
          We compare with four one-shot methods: <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413532" target="_blank">Wav2Lip</a> (ACM MM 2020), 
          <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25464" target="_blank">DINet</a> (AAAI 2023), 
          <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhong_Identity-Preserving_Talking_Face_Generation_With_Landmark_and_Appearance_Priors_CVPR_2023_paper.html">IP-LAP</a> (CVPR 2023), 
          and <a href="https://link.springer.com/chapter/10.1007/978-3-031-72658-3_23" target="_blank">EDTalk</a> (ECCV 2024). Our SyncAnimation achieves dynamic and clear lip shapes, as well as lip-audio consistency.  
        </h4>
      
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/May_gan_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/RonWyden1_gan_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/DebFischer0_gan_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/ChrisMurphy1_gan_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
      
      <h3 class="title is-4" style="text-align: left; margin: 0; padding: 20px 0px;">Comparison with Nerf</h3>
          <!-- We compare with 三个 one-shot methods在: ER-NeRF, SyncTalk (CVPR 2023) and GeneFace++ 。我们的SyncAnimation实现了具有强语音相关性的avatar姿势和表情
          --> 
        <h4 class="text-align: justify" style="width: 100%; text-align: justify; margin-bottom: 5px;">
          We compare with three one-shot methods: <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.html?ref=https://githubhelp.com" target="_blank">ER-NeRF</a> (ICCV 2023), 
          <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Peng_SyncTalk_The_Devil_is_in_the_Synchronization_for_Talking_Head_CVPR_2024_paper.html" target="_blank">SyncTalk</a> (CVPR 2024),  
          and <a href="https://arxiv.org/abs/2305.00787" target="_blank">GeneFace++</a> (arXiv 2023). SyncAnimation is the only method that successfully achieves avatar poses and expressions with strong audio correlation.
        </h4>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/May_nerf_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/RonWyden1_nerf_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/DebFischer0_nerf_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/ChrisMurphy1_nerf_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
      
      <h3 class="title is-4" style="text-align: left; margin: 0; padding: 20px 0px;">Comparison with Stable Diffusion</h3>
        <!-- We compare with 三个 one-shot methods: Hallo, V-Express (CVPR 2023) and EchoMimic 。我们的SyncAnimation实现逼真的avatar，
             拥有更多的面部细节，和更丰富的姿势和表情
          --> 
        <h4 class="text-align: justify" style="width: 100%; text-align: justify; margin-bottom: 5px;">
          We compare with three one-shot methods: <a href="https://arxiv.org/abs/2406.08801" target="_blank">Hallo</a> (arXiv 2024), 
          <a href="https://arxiv.org/abs/2406.02511" target="_blank">V-Express</a> (arXiv 2024),  
          and <a href="https://arxiv.org/abs/2407.08136" target="_blank">EchoMimic</a> (arxiv 2024). Our SyncAnimation generates realistic avatars with more facial details and richer poses and expressions.
        </h4>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/May_sd_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/RonWyden1_sd_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/DebFischer0_sd_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
        <video id="teaser" controls playsinline height="100%">
          <source src="./static/videos/ChrisMurphy1_sd_vs_ours_with_bottom_border_and_title.mp4" type="video/mp4">
        </video>
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser" style="box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3); padding: 20px; border-radius: 8px; width: 70%; margin: 0 auto;margin-bottom: 30px;">
  <div class="container is-max-desktop" style="padding: 0; margin: 0;">
    <div class="hero-body">
      <!-- h2 居中对齐 -->
      <h2 class="title is-3" style="font-size: 2.1em; font-weight: bold; margin-bottom: 20px; text-align: center;">Torso Scaling Expansion</h2>

      <h3 class="text-align: justify" style="width: 100%; text-align: justify; margin-bottom: 5px;">
        We deviate from the conventional approach of restricting upper-body scale and reject the practice of pasting back to the original frame. 
        Our method, SyncAnimation, directly renders four upper-body avatars driven by four audio segments, ensuring strong audio correlation and natural, unrestricted poses and facial expressions.
      </h3>
      
      <div style="display: flex; justify-content: space-between; gap: 5px; max-width: 100%; margin: 0 auto; box-sizing: border-box;">
          <video id="teaser1" controls playsinline style="height: 50%; width: 48%; max-width: 48%;">
            <source src="./static/videos/3-DebFischer0+May.mp4" type="video/mp4">
          </video>
          <video id="teaser2" controls playsinline style="height: 50%; width: 48%; max-width: 48%;">
            <source src="./static/videos/4-DebFischer0+May.mp4" type="video/mp4">
          </video>
        </div>
      <div style="display: flex; justify-content: space-between; gap: 5px; max-width: 100%; margin-top: 20px;; box-sizing: border-box;">
          <video id="teaser1" controls playsinline style="height: 50%; width: 48%; max-width: 48%;">
            <source src="./static/videos/1-ChrisVanHollen1+RonWyden1.mp4" type="video/mp4">
          </video>
          <video id="teaser2" controls playsinline style="height: 50%; width: 48%; max-width: 48%;">
            <source src="./static/videos/2-ChrisVanHollen1+RonWyden1.mp4" type="video/mp4">
          </video>
        </div>
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser" style="box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3); padding: 20px; border-radius: 8px; width: 70%; margin: 0 auto;margin-bottom: 30px;">
  <div class="container is-max-desktop" style="padding: 0; margin: 0;">
    <div class="hero-body">
      <!-- h2 居中对齐 -->
      <h2 class="title is-3" style="font-size: 2.1em; font-weight: bold; margin-bottom: 20px; text-align: center;">Live News</h2>
        <!-- 在新闻报道或电视广播中，两个记者或被采访人通常会以对谈或讨论的方式进行交互，在对方进行说话时，己方需要保持短暂的“静止”，反之亦然。
          实现这一过程不仅需要avatar具有更大的upper-body 渲染，因为人物通常不会只以头的形式出现，也需要avatar的姿态和表情与音频高度相关，
          因为“静止”时段需要对移动和表情进行限制。我们从两段新闻对话中，分别提取了男声和女声，结合ours SyncAnimation渲染生成了4个具有
          upper-body的avatars with strong audio relation。视频中展示了所渲染的avatar能够很好的完成这一对话/采访能力，
          证明ours SyncAnimation所具有的优秀的音频驱动姿态和面部表情能力和更大的躯干渲染能力。 -->
        <h3 class="text-align: justify" style="width: 100%; text-align: justify; margin-bottom: 5px;">
            In news reporting or television broadcasts, two journalists or interviewees typically engage in interaction 
            through dialogue or discussion, where one party remains momentarily "still" while the other speaks, and vice versa. 
            However, achieving this requires not only rendering a larger upper body for the avatar, as individuals are often not represented solely by their heads, 
            but also ensuring that the avatar's poses and expressions are highly correlated with the audio, 
            since "still" periods require constraining both movement and expression.
            From two segments of different news dialogues, we extracted the male and female audio tracks and used our SyncAnimation method to render four avatars with upper bodies and strong audio correlation. 
            The first row displays avatars rendered by SyncAnimation, while the second row shows the actual dialogue or discussion.
            The videos demonstrate how the rendered avatars successfully perform the dialogue/interview task, 
            proving the excellent audio-driven pose and facial expression capabilities of our SyncAnimation, along with its ability to render upper body.
        </h3>
        
        <div style="display: flex; justify-content: space-between; gap: 10px; max-width: 100%; margin: 0 auto; box-sizing: border-box;">
          <video id="teaser1" controls playsinline style="height: 50%; width: 48%; max-width: 48%;">
            <source src="./static/videos/5-DebFischer0+RonWyden1-com.mp4" type="video/mp4">
          </video>
          <video id="teaser2" controls playsinline style="height: 50%; width: 48%; max-width: 48%;">
            <source src="./static/videos/6-May+ChrisVanHollen1-com.mp4" type="video/mp4">
          </video>
        </div>
      </h2>
    </div>
  </div>
</section>


  
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3" style="text-align: center;">Comparison with SOTA</h2>
<!--       <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
<!--         portraits. -->
      </h2>
    </div>
  </div>
</section> -->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3" style="text-align: center;">Torso Scaling Expansion</h2>
<!--       <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
<!--         portraits. -->
      </h2>
    </div>
  </div>
</section> -->
  
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3" style="text-align: center;">Live News</h2>
<!--       <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
<!--         portraits. -->
      </h2>
    </div>
  </div>
</section> -->

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
  </div>
</footer>

</body>
</html>
